# Historia ClÃ­nica Distribuida con Citus en Minikube

Este proyecto implementa un sistema de historia clÃ­nica distribuida utilizando **PostgreSQL Citus** como base de datos distribuida y **Flask** como middleware API, todo desplegado en **Kubernetes** con **Minikube**.

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Middleware    â”‚â”€â”€â”€â–¶â”‚  Citus Coordinator â”‚â”€â”€â”€â–¶â”‚  Citus Worker 1 â”‚
â”‚  (Flask API)    â”‚    â”‚   (PostgreSQL)    â”‚    â”‚  (PostgreSQL)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Citus Worker 2 â”‚
                       â”‚  (PostgreSQL)   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes:
- **Citus Coordinator**: Nodo coordinador que maneja las consultas distribuidas
- **Citus Workers**: Nodos trabajadores que almacenan los datos distribuidos
- **Middleware Flask**: API REST que expone endpoints para interactuar con la base de datos
- **Kubernetes**: Orquestador de contenedores para el despliegue

## ğŸ“‹ Prerrequisitos

### Software Requerido:
- **Docker Desktop** o **Podman** (para contenedores)
- **Minikube** (para Kubernetes local)
- **kubectl** (cliente de Kubernetes)
- **curl** (para pruebas de API)
- **WSL2** (en Windows, opcional pero recomendado)

### InstalaciÃ³n de Prerrequisitos en Windows:

```powershell
# Instalar Chocolatey (gestor de paquetes)
Set-ExecutionPolicy Bypass -Scope Process -Force
[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072
iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))

# Instalar herramientas necesarias
choco install minikube kubernetes-cli docker-desktop -y
```

### En Linux/WSL2:
```bash
# Instalar Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Instalar kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Instalar Minikube
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube /usr/local/bin/
```

## ğŸš€ InstalaciÃ³n y Despliegue

### 1. Clonar el Repositorio
```bash
git clone https://github.com/jaiderreyes/historia-clinica-distribuida-minikube.git
cd historia-clinica-distribuida-minikube
```

### 2. Iniciar Minikube
```bash
# Iniciar Minikube con Docker como driver
minikube start --driver=docker

# Verificar que estÃ¡ funcionando
minikube status
kubectl get nodes
```

### 3. Desplegar ConfigMaps
```bash
# Aplicar configuraciones SQL y del middleware
kubectl apply -f configmap-sqls.yaml
kubectl apply -f middleware-configmap.yaml
```

### 4. Desplegar Citus Coordinator
```bash
# Desplegar el nodo coordinador
kubectl apply -f coordinator-deployment.yaml

# Esperar a que estÃ© listo (puede tardar varios minutos)
kubectl wait --for=condition=ready --timeout=300s pod -l app=citus-coordinator

# Verificar logs de inicializaciÃ³n
kubectl logs -l app=citus-coordinator
```

### 5. Desplegar Citus Workers
```bash
# Desplegar los nodos trabajadores
kubectl apply -f worker1-deployment.yaml
kubectl apply -f worker2-deployment.yaml

# Esperar a que estÃ©n listos
kubectl wait --for=condition=ready --timeout=300s pod -l app=citus-worker

# Verificar que todos los pods estÃ©n funcionando
kubectl get pods
```

### 6. Configurar el Cluster Citus
```bash
# Conectar workers al coordinador
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "SELECT * FROM master_add_node('citus-worker-1', 5432);"
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "SELECT * FROM master_add_node('citus-worker-2', 5432);"

# Verificar workers conectados
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "SELECT * FROM master_get_active_worker_nodes();"
```

### 7. Distribuir las Tablas
```bash
# Crear tablas de referencia (replicadas en todos los nodos)
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "SELECT create_reference_table('medicos');"
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "SELECT create_reference_table('usuario');"

# Distribuir tablas principales
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "SELECT create_distributed_table('pacientes', 'cedula');"
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "SELECT create_distributed_table('consultas', 'paciente_id');"

# Verificar distribuciÃ³n de tablas
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "SELECT table_name, citus_table_type FROM citus_tables;"
```

### 8. Desplegar Middleware
```bash
# Desplegar el middleware Flask
kubectl apply -f middleware-deployment.yaml
kubectl apply -f middleware-service.yaml

# Esperar a que estÃ© listo
kubectl wait --for=condition=ready --timeout=300s pod -l app=middleware

# Ver logs del middleware
kubectl logs -f -l app=middleware
```

### 9. Acceder al Sistema
```bash
# Hacer port-forward para acceder desde localhost
kubectl port-forward service/citus-coordinator 15432:5432 &
kubectl port-forward service/middleware-service 8080:8080 &

# Verificar que los port-forwards estÃ©n funcionando
jobs
```

## ğŸ§ª Pruebas del Sistema

### API REST del Middleware
```bash
# Health check
curl http://localhost:8080

# Estado de la base de datos
curl http://localhost:8080/status

# Obtener usuarios
curl http://localhost:8080/usuarios

# Ver workers de Citus
curl http://localhost:8080/citus/workers
```

### Consultas Directas a la Base de Datos
```bash
# Conectar directamente al coordinador
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica

# Consulta distribuida de ejemplo
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "
SELECT 
    p.nombre, p.apellido, p.ciudad,
    m.nombre as medico_nombre, m.especialidad,
    c.diagnostico, c.fecha_consulta
FROM pacientes p
JOIN consultas c ON p.id = c.paciente_id
JOIN medicos m ON c.medico_id = m.id
LIMIT 5;"

# Ver estadÃ­sticas
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "SELECT COUNT(*) as total_pacientes FROM pacientes;"
kubectl exec -it deployment/citus-coordinator -- psql -U admin -d historia_clinica -c "SELECT COUNT(*) as total_consultas FROM consultas;"
```

## ğŸ“Š Estructura de la Base de Datos

### Tablas Distribuidas:
- **pacientes**: Distribuida por `cedula`
  - Contiene informaciÃ³n bÃ¡sica de pacientes
  - Particionada por cÃ©dula para distribuciÃ³n eficiente

- **consultas**: Distribuida por `paciente_id`
  - Almacena consultas mÃ©dicas
  - Co-ubicada con pacientes para consultas eficientes

### Tablas de Referencia:
- **medicos**: Replicada en todos los nodos
  - InformaciÃ³n de mÃ©dicos
  - Acceso rÃ¡pido desde cualquier nodo

- **usuario**: Replicada en todos los nodos
  - Usuarios del sistema
  - Datos de autenticaciÃ³n y roles

## ğŸ”§ Endpoints de la API

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| GET | `/` | Health check del middleware |
| GET | `/status` | Estado de conexiÃ³n con la base de datos |
| GET | `/usuarios` | Lista de usuarios del sistema |
| GET | `/citus/workers` | Lista de workers activos en Citus |

## ğŸ“ Archivos del Proyecto

```
historia-clinica-distribuida-minikube/
â”œâ”€â”€ README.md                      # Este archivo
â”œâ”€â”€ configmap-sqls.yaml           # Scripts SQL de inicializaciÃ³n
â”œâ”€â”€ middleware-configmap.yaml      # CÃ³digo Python del middleware
â”œâ”€â”€ coordinator-deployment.yaml    # Deployment del coordinador Citus
â”œâ”€â”€ worker1-deployment.yaml       # Deployment del worker 1
â”œâ”€â”€ worker2-deployment.yaml       # Deployment del worker 2
â”œâ”€â”€ middleware-deployment.yaml     # Deployment del middleware Flask
â”œâ”€â”€ middleware-service.yaml       # Servicio del middleware
â””â”€â”€ schema-init-job.yaml          # Job de inicializaciÃ³n (opcional)
```

## ğŸ› SoluciÃ³n de Problemas

### Problema: Pod en estado "Pending" o "ContainerCreating"
```bash
# Verificar recursos del cluster
kubectl describe node minikube
kubectl get events --sort-by=.metadata.creationTimestamp | tail -10

# Reiniciar Minikube si es necesario
minikube delete && minikube start --driver=docker
```

### Problema: Error de conexiÃ³n a la base de datos
```bash
# Verificar que todos los pods estÃ©n corriendo
kubectl get pods

# Ver logs detallados
kubectl describe pod -l app=citus-coordinator
kubectl logs -l app=citus-coordinator
```

### Problema: Puerto 5432 ya en uso
```bash
# Usar puerto diferente para port-forward
kubectl port-forward service/citus-coordinator 15432:5432 &
```

### Problema: Middleware no responde
```bash
# Verificar logs del middleware
kubectl logs -l app=middleware

# Reiniciar deployment
kubectl rollout restart deployment/middleware-citus
```

## ğŸ” Comandos Ãštiles de DiagnÃ³stico

```bash
# Ver estado general
kubectl get all

# Ver eventos recientes
kubectl get events --sort-by=.metadata.creationTimestamp

# Ver logs de un pod especÃ­fico
kubectl logs <pod-name>

# Ejecutar comandos dentro de un pod
kubectl exec -it <pod-name> -- /bin/bash

# Ver descripciÃ³n detallada de recursos
kubectl describe pod <pod-name>
kubectl describe deployment <deployment-name>

# Ver uso de recursos
kubectl top nodes
kubectl top pods
```

## ğŸ§¹ Limpieza del Sistema

### Eliminar todos los recursos del proyecto:
```bash
kubectl delete -f .
```

### Eliminar Minikube completamente:
```bash
minikube delete
```

### Reiniciar desde cero:
```bash
minikube delete
minikube start --driver=docker
# Luego seguir los pasos de despliegue
```

## ğŸ“ˆ CaracterÃ­sticas del Sistema

### Escalabilidad:
- âœ… Base de datos distribuida horizontalmente
- âœ… Capacidad de agregar mÃ¡s workers
- âœ… Particionamiento automÃ¡tico de datos

### Alta Disponibilidad:
- âœ… ReplicaciÃ³n de tablas de referencia
- âœ… Tolerancia a fallas de workers individuales
- âœ… Reinicio automÃ¡tico de contenedores

### Rendimiento:
- âœ… Consultas paralelas en mÃºltiples nodos
- âœ… Co-ubicaciÃ³n de datos relacionados
- âœ… Ãndices distribuidos

## ğŸ¤ Contribuciones

Para contribuir al proyecto:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-caracteristica`)
3. Commit tus cambios (`git commit -am 'Agregar nueva caracterÃ­stica'`)
4. Push a la rama (`git push origin feature/nueva-caracteristica`)
5. Crea un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ‘¥ Autores

- **Jaider Reyes** - Desarrollo inicial - [jaiderreyes](https://github.com/jaiderreyes)

## ğŸ™ Agradecimientos

- [Citus Data](https://github.com/citusdata/citus) por la extensiÃ³n de PostgreSQL
- [Kubernetes](https://kubernetes.io/) por la plataforma de orquestaciÃ³n
- [Flask](https://flask.palletsprojects.com/) por el framework web de Python

Este laboratorio implementa una soluciÃ³n de base de datos distribuida para almacenar historias clÃ­nicas usando PostgreSQL con la extensiÃ³n Citus. El despliegue se realiza sobre Minikube (entorno Kubernetes local), simulando un entorno productivo y escalable.

---

##  Objetivo

- Desplegar un clÃºster PostgreSQL-Citus (1 Coordinator + 2 Workers)
- Fragmentar automÃ¡ticamente la tabla `usuario` por `documento_id`
- Exponer un middleware Python como cliente para realizar consultas distribuidas
- Integrar este laboratorio con prÃ¡cticas de Gobierno del Dato

---

## Requisitos

- [Minikube](https://minikube.sigs.k8s.io/)
- `kubectl` configurado localmente
- Docker funcionando
- Python 3 (para pruebas locales opcionales)

---

## Estructura

```
k8s/
â”œâ”€â”€ coordinator-deployment.yaml
â”œâ”€â”€ worker1-deployment.yaml
â”œâ”€â”€ worker2-deployment.yaml
â”œâ”€â”€ configmap-sqls.yaml
â”œâ”€â”€ schema-init-job.yaml
â”œâ”€â”€ middleware-deployment.yaml
â””â”€â”€ middleware-configmap.yaml
```

---

## Instrucciones de despliegue

1. **Inicia Minikube:**

```bash
minikube start --cpus=4 --memory=6g
```

2. **Aplica los manifiestos del clÃºster:**

```bash
kubectl apply -f k8s/configmap-sqls.yaml
kubectl apply -f k8s/coordinator-deployment.yaml
kubectl apply -f k8s/worker1-deployment.yaml
kubectl apply -f k8s/worker2-deployment.yaml
```

3. **Inicializa el esquema y los datos:**

```bash
kubectl apply -f k8s/schema-init-job.yaml
```

4. **Despliega el middleware en Python:**

```bash
kubectl apply -f k8s/middleware-configmap.yaml
kubectl apply -f k8s/middleware-deployment.yaml
```

5. **Verifica los logs del middleware:**

```bash
kubectl logs deploy/middleware-citus
```

DeberÃ­as ver un listado de usuarios cargados desde la tabla `usuario`.

---

## Notas adicionales

- Puedes escalar horizontalmente los workers con:

```bash
kubectl scale deploy citus-worker-1 --replicas=2
```

- Este laboratorio puede extenderse integrando Prometheus, Grafana o FastAPI para observabilidad y API REST.

---

## Uso en el aula

Este laboratorio estÃ¡ pensado para clases de:
- Bases de datos distribuidas
- Gobierno del dato
- Interoperabilidad en salud
- Arquitectura cloud-native

---

## ğŸ“˜ Licencia

MIT Â© 2025 â€“ Proyecto acadÃ©mico para fines educativos.
#   K 8 - S I S  
 